# -*- coding: utf-8 -*-
"""Potato_Disease_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cv1sG20qZI9_fZADVjKFufvFt7aCrbhZ
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import os
import pandas as pd

# Base path to dataset
drive_path = "/content/drive/MyDrive/Potato"
subfolders = ["Train", "Test", "Valid"]  # These contain the actual class folders

image_paths = []
labels = []
split_info = []  # To record whether it's from Train, Test, or Valid

# Traverse each split folder
for split in subfolders:
    split_path = os.path.join(drive_path, split)

    # Loop through category folders inside each split
    if os.path.exists(split_path):
        for category in os.listdir(split_path):
            category_path = os.path.join(split_path, category)

            if not os.path.isdir(category_path):
                continue  # Skip files

            for image_name in os.listdir(category_path):
                image_path = os.path.join(category_path, image_name)
                image_paths.append(image_path)
                labels.append(category)
                split_info.append(split)
    else:
        print(f"⚠️ Split folder not found: {split_path}")

# Create DataFrame
df = pd.DataFrame({
    "image_path": image_paths,
    "label": labels,
    "split": split_info
})

# Display first few rows
df.head()

df.tail()

df.shape

df.columns

df.duplicated().sum()

df.isnull().sum()

df.info()

df['label'].unique()

df['label'].value_counts()

# ✅ Mount Google Drive if needed (skip if dataset is local or on Kaggle)
# from google.colab import drive
# drive.mount('/content/drive')

# ✅ Standard Imports
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report

import tensorflow as tf
from tensorflow.keras.utils import plot_model
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Lambda, Conv2D, MaxPooling2D, Flatten,
                                     Dense, Reshape, Concatenate, Dot, Softmax, Multiply)
from tensorflow.keras.preprocessing.image import ImageDataGenerator

"""Load and Process Dataset"""

# ✅ Modify the path below to your Colab or Drive path
base_path = "/content/drive/MyDrive/Potato/Train"  # adjust as needed

categories = os.listdir(base_path)
image_paths, labels = [], []

for category in categories:
    category_path = os.path.join(base_path, category)
    if not os.path.isdir(category_path):
        continue
    for img_name in os.listdir(category_path):
        img_path = os.path.join(category_path, img_name)
        image_paths.append(img_path)
        labels.append(category)

df = pd.DataFrame({'image_path': image_paths, 'label': labels})

"""Clean & Balance Data"""

# Remove corrupt images
def verify_images(df):
    good = []
    for path in df['image_path']:
        try:
            img = Image.open(path)
            img.verify()
            good.append(path)
        except:
            continue
    return df[df['image_path'].isin(good)].reset_index(drop=True)

df = verify_images(df)

# Encode labels
le = LabelEncoder()
df['category_encoded'] = le.fit_transform(df['label'])

# Balance classes
max_count = df['category_encoded'].value_counts().max()
dfs = []
for c in df['category_encoded'].unique():
    class_df = df[df['category_encoded'] == c]
    upsampled = resample(class_df, replace=True, n_samples=max_count, random_state=42)
    dfs.append(upsampled)
df_resampled = pd.concat(dfs).sample(frac=1, random_state=42).reset_index(drop=True)

"""Train-Test Split and Generators"""

# Split
train_df_new, temp_df_new = train_test_split(df_resampled, train_size=0.8, stratify=df_resampled['category_encoded'], random_state=42)
valid_df_new, test_df_new = train_test_split(temp_df_new, test_size=0.5, stratify=temp_df_new['category_encoded'], random_state=42)

# Image generators
img_size = (224, 224)
batch_size = 16

train_gen_new = ImageDataGenerator(rescale=1./255).flow_from_dataframe(
    train_df_new, x_col='image_path', y_col='label',
    target_size=img_size, class_mode='sparse', color_mode='rgb',
    shuffle=True, batch_size=batch_size)

valid_gen_new = ImageDataGenerator(rescale=1./255).flow_from_dataframe(
    valid_df_new, x_col='image_path', y_col='label',
    target_size=img_size, class_mode='sparse', color_mode='rgb',
    shuffle=True, batch_size=batch_size)

test_gen_new = ImageDataGenerator(rescale=1./255).flow_from_dataframe(
    test_df_new, x_col='image_path', y_col='label',
    target_size=img_size, class_mode='sparse', color_mode='rgb',
    shuffle=False, batch_size=batch_size)

num_classes = df['label'].nunique()

"""Build Model (Dual-Branch with Attention)"""

def split_image(x):
    upper = x[:, :img_size[0]//2, :, :]
    lower = x[:, img_size[0]//2:, :, :]
    return [upper, lower]

def flip_image(x):
    return tf.image.flip_left_right(x)

input_layer = Input(shape=(224, 224, 3))
upper_half, lower_half = Lambda(split_image)(input_layer)
lower_half_flipped = Lambda(flip_image)(lower_half)

# Upper branch
upper = Conv2D(32, 3, padding='same', activation='relu')(upper_half)
upper = MaxPooling2D(2)(upper)
upper = Conv2D(64, 3, padding='same', activation='relu')(upper)
upper = MaxPooling2D(2)(upper)
upper = Conv2D(128, 3, padding='same', activation='relu')(upper)
upper = MaxPooling2D(2)(upper)
upper = Flatten()(upper)
upper = Dense(512, activation='relu')(upper)
upper = Reshape((1, 512))(upper)

# Lower branch
lower = Conv2D(32, 3, padding='same', activation='relu')(lower_half_flipped)
lower = MaxPooling2D(2)(lower)
lower = Conv2D(64, 3, padding='same', activation='relu')(lower)
lower = MaxPooling2D(2)(lower)
lower = Conv2D(128, 3, padding='same', activation='relu')(lower)
lower = MaxPooling2D(2)(lower)
lower = Flatten()(lower)
lower = Dense(512, activation='relu')(lower)
lower = Reshape((1, 512))(lower)

# Attention
similarity = Dot(axes=-1, normalize=True)([upper, lower])
attention = Softmax(axis=1)(similarity)
attended = Multiply()([attention, upper])
combined = Concatenate(axis=-1)([attended, lower])
combined = Reshape((1024,))(combined)

# Classifier
x = Dense(256, activation='relu')(combined)
x = Dense(128, activation='relu')(x)
output = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_layer, outputs=output)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

"""Train & Evaluate"""

# Train model
history = model.fit(
    train_gen_new,
    validation_data=valid_gen_new,
    epochs=3
)

# Plot results
def plot_history(history):
    plt.figure(figsize=(12,5))
    plt.subplot(1,2,1)
    plt.plot(history.history['accuracy'], label='train')
    plt.plot(history.history['val_accuracy'], label='val')
    plt.title('Accuracy')
    plt.legend()

    plt.subplot(1,2,2)
    plt.plot(history.history['loss'], label='train')
    plt.plot(history.history['val_loss'], label='val')
    plt.title('Loss')
    plt.legend()

    plt.show()

plot_history(history)

"""Test Results"""

# Test set evaluation
test_loss, test_acc = model.evaluate(test_gen_new)
print(f"✅ Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}")

# Confusion matrix and classification report
y_pred = model.predict(test_gen_new)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = test_gen_new.classes
labels = list(test_gen_new.class_indices.keys())

cm = confusion_matrix(y_true, y_pred_classes)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

print("Classification Report:\n")
print(classification_report(y_true, y_pred_classes, target_names=labels))



